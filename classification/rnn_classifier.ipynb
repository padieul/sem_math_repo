{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# takes care of annoying TF-GPU warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very useful for managing wandb runs: https://stackoverflow.com/questions/71106179/log-two-model-runs-with-keras-wandb\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN: Formula Label Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import utils\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import datetime\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[\"val_\"+metric], \"\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, \"val_\"+metric])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data and Preprocess Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(corpus,\n",
    "                    irrelevant_features=[\"mtype\",]):\n",
    "    # drop irrelevant columns\n",
    "    corpus.drop(irrelevant_features, inplace=True, axis=1)\n",
    "\n",
    "    # filter strings\n",
    "    def process_cell(cell_str):\n",
    "        stripped_f_str = cell_str[1:-1].replace(\"\\\\\\\\\", \"\\\\\")\n",
    "        f_list = stripped_f_str.split(\",\")\n",
    "        f_list = [token.replace(\"'\", \"\").replace(\" \", \"\") for token in f_list]\n",
    "        f_list = [\"{\" if token == \"\\\\{\" else token for token in f_list]\n",
    "        f_list = [\"}\" if token == \"\\\\}\" else token for token in f_list]\n",
    "        cell_str = \" \".join(f_list)\n",
    "        return cell_str\n",
    "\n",
    "    corpus[\"tokens\"] = corpus[\"tokens\"].map(process_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = Path(\"data/\") / \"bin_class_data_TOKENIZED_V1.csv\"\n",
    "data = pd.read_csv(data_p)\n",
    "preprocess_data(data)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = 11000\n",
    "data_as_ds = tf.data.Dataset.from_tensor_slices((data[\"tokens\"], data[\"label\"])) \n",
    "test_dataset = data_as_ds.take(1000)\n",
    "train_dataset = data_as_ds.skip(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "    print(\"text: \", example.numpy())\n",
    "    print(\"label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(output_mode_str, n_grams):\n",
    "    VOCAB_SIZE = 1000\n",
    "    encoder = tf.keras.layers.TextVectorization(\n",
    "        standardize=None,\n",
    "        output_mode=output_mode_str,\n",
    "        ngrams = n_grams,\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=VOCAB_SIZE)\n",
    "    encoder.adapt(train_dataset.map(lambda tokens, label: tokens)) # removes the label column through transformation: text, label -> text\n",
    "    return encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 1: Use integer indices encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_int = create_encoder(\"int\", None)\n",
    "vocab = np.array(encoder_int.get_vocabulary())\n",
    "vocab[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder_int(example).numpy()\n",
    "print(example)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original: \", example.numpy())\n",
    "print(\"After reverse lookup: \", \" \".join(vocab[encoded_example]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 2: Binary Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_multi_hot = create_encoder(\"multi_hot\", None)\n",
    "vocab = np.array(encoder_multi_hot.get_vocabulary())\n",
    "vocab[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder_multi_hot(example).numpy()\n",
    "print(example)\n",
    "print(encoded_example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 3: Frequency Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_count = create_encoder(\"count\", None)\n",
    "vocab = np.array(encoder_count.get_vocabulary())\n",
    "vocab[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder_count(example).numpy()\n",
    "print(example)\n",
    "print(encoded_example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 4: Bigrams Frequency Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_bigrams = create_encoder(\"count\", (2))\n",
    "vocab = np.array(encoder_bigrams.get_vocabulary())\n",
    "vocab[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder_bigrams(example).numpy()\n",
    "print(example)\n",
    "print(encoded_example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 5: Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_tf_idf = create_encoder(\"tf_idf\", None)\n",
    "vocab = np.array(encoder_tf_idf.get_vocabulary())\n",
    "vocab[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder_tf_idf(example).numpy()\n",
    "print(example)\n",
    "print(encoded_example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model (Bidirectional LSTM with one layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Define and compile model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vec_encoder):\n",
    "    model = tf.keras.Sequential([\n",
    "        vec_encoder,\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=len(vec_encoder.get_vocabulary()),\n",
    "            output_dim=64,\n",
    "            # user masking to handle the variable sequence lengths\n",
    "            mask_zero=True),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Train the model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 1: Use integer indices for encoding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TENSORBOARD USAGE\n",
    "#log_dir = \"4_nlp_rnns/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(encoder_int)\n",
    "# predict on a sample formula using untrained model\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"binary_precalculus_simple\", reinit=True)\n",
    "wandb.config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"int\"\n",
    "}\n",
    "\n",
    "history = model.fit(train_dataset, epochs=15,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30,callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 2: Use binary count vectorizer for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(encoder_multi_hot)\n",
    "# predict on a sample formula using untrained model\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"binary_precalculus_simple\", reinit=True)\n",
    "wandb.config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"multi_hot\"\n",
    "}\n",
    "\n",
    "history = model.fit(train_dataset, epochs=15,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30,callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(encoder_count)\n",
    "# predict on a sample formula using untrained model\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"binary_precalculus_simple\", reinit=True)\n",
    "wandb.config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"count\"\n",
    "}\n",
    "\n",
    "history = model.fit(train_dataset, epochs=15,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30,callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(encoder_bigrams)\n",
    "# predict on a sample formula using untrained model\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"binary_precalculus_simple\", reinit=True)\n",
    "wandb.config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"bigram_count\"\n",
    "}\n",
    "\n",
    "history = model.fit(train_dataset, epochs=15,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30,callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(encoder_tf_idf)\n",
    "# predict on a sample formula using untrained model\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"binary_precalculus_simple\", reinit=True)\n",
    "wandb.config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"tf_idf\"\n",
    "}\n",
    "\n",
    "history = model.fit(train_dataset, epochs=15,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30,callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem_math_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec  7 2022, 01:12:00) [GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e475bf32faa723a86579ec24c4297aa7888421b748796abf349c5564eb9c10ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
