{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# takes care of annoying TF-GPU warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very useful for managing wandb runs: https://stackoverflow.com/questions/71106179/log-two-model-runs-with-keras-wandb\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN: Formula Label Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import datetime\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "wandb_project_name = \"binary_formula_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[\"val_\"+metric], \"\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, \"val_\"+metric])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data and Preprocess Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(corpus,\n",
    "                    irrelevant_features=[\"mtype\",]):\n",
    "    # drop irrelevant columns\n",
    "    corpus.drop(irrelevant_features, inplace=True, axis=1)\n",
    "\n",
    "    # filter strings\n",
    "    def process_cell(cell_str):\n",
    "        stripped_f_str = cell_str[1:-1].replace(\"\\\\\\\\\", \"\\\\\")\n",
    "        f_list = stripped_f_str.split(\",\")\n",
    "        f_list = [token.replace(\"'\", \"\").replace(\" \", \"\") for token in f_list]\n",
    "        f_list = [\"{\" if token == \"\\\\{\" else token for token in f_list]\n",
    "        f_list = [\"}\" if token == \"\\\\}\" else token for token in f_list]\n",
    "        cell_str = \" \".join(f_list)\n",
    "        return cell_str\n",
    "\n",
    "    corpus[\"tokens\"] = corpus[\"tokens\"].map(process_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "data_p = Path(\"../data/\") / \"bin_class_data_TOKENIZED_SET_V1.csv\"\n",
    "data = pd.read_csv(data_p)\n",
    "preprocess_data(data)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_TRAIN_DS = 10000\n",
    "SIZE_TEST_DS = 1000\n",
    "data_as_ds = tf.data.Dataset.from_tensor_slices((data[\"tokens\"], data[\"label\"])) \n",
    "test_dataset = data_as_ds.take(SIZE_TEST_DS)\n",
    "train_dataset = data_as_ds.skip(SIZE_TEST_DS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, label in train_dataset.take(3):\n",
    "    print(\"text: \", example.numpy())\n",
    "    print(\"label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = SIZE_TRAIN_DS\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCH = np.floor(SIZE_TRAIN_DS/BATCH_SIZE)\n",
    "VAL_STEPS_PER_EPOCH = np.floor(SIZE_TEST_DS/BATCH_SIZE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(output_mode_str, n_grams):\n",
    "       \n",
    "    VOCAB_SIZE = 1000\n",
    "    encoder = tf.keras.layers.TextVectorization(\n",
    "        standardize=None,\n",
    "        output_mode=output_mode_str,\n",
    "        ngrams = n_grams,\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=VOCAB_SIZE)\n",
    "    encoder.adapt(train_dataset.map(lambda tokens, label: tokens)) # removes the label column through transformation: text, label -> text\n",
    "    return encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 1: Use integer indices encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_int = create_encoder(\"int\", None)\n",
    "vocab = np.array(encoder_int.get_vocabulary())\n",
    "vocab[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder_int(example).numpy()\n",
    "print(example)\n",
    "print(encoded_example)\n",
    "print(encoded_example.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original: \", example.numpy())\n",
    "print(\"After reverse lookup: \", \" \".join(vocab[encoded_example]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputter = tf.keras.layers.Input(shape=(len(encoder_int.get_vocabulary()),) ,batch_size = 1)\n",
    "#pooled_val = tf.reshape(embedded_val, [val1*val2, 1])\n",
    "#encoded_example = tf.reshape(encoded_example, [1,len(encoder_int.get_vocabulary())-1])\n",
    "#print(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "embedder = tf.keras.layers.Embedding(\n",
    "            input_dim=len(encoder_int.get_vocabulary()),\n",
    "            output_dim=64,\n",
    "            # user masking to handle the variable sequence lengths\n",
    "            mask_zero=True)\n",
    "embedded_val = embedder(encoded_example)\n",
    "print(embedded_val)\n",
    "embedded_val = tf.reshape(embedded_val, [1,122,64])\n",
    "#print(embedded_val.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "flatter = tf.keras.layers.Flatten(input_shape=(122, 64))\n",
    "flattened = flatter(embedded_val)\n",
    "print(flattened.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val1, val2 = embedded_val.shape\n",
    "#pooled_val = tf.reshape(embedded_val, [val1*val2, 1])\n",
    "#print(pooled_val)\n",
    "#pooled_val = tf.reshape(embedded_val, [122, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denser = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "#denser_val = denser(pooled_val)\n",
    "#print(denser_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 2: Binary Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_multi_hot = create_encoder(\"multi_hot\", None)\n",
    "vocab = np.array(encoder_multi_hot.get_vocabulary())\n",
    "vocab[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder_multi_hot(example).numpy()\n",
    "print(example)\n",
    "print(encoded_example)\n",
    "print(encoded_example.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 3: Frequency Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_count = create_encoder(\"count\", None)\n",
    "vocab = np.array(encoder_count.get_vocabulary())\n",
    "vocab[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder_count(example).numpy()\n",
    "print(example)\n",
    "print(encoded_example)\n",
    "print(encoded_example.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 4: Bigrams Frequency Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_bigrams = create_encoder(\"count\", (2))\n",
    "vocab = np.array(encoder_bigrams.get_vocabulary())\n",
    "vocab[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder_bigrams(example).numpy()\n",
    "print(example)\n",
    "print(encoded_example)\n",
    "print(encoded_example.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 5: Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_tf_idf = create_encoder(\"tf_idf\", None)\n",
    "vocab = np.array(encoder_tf_idf.get_vocabulary())\n",
    "vocab[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder_tf_idf(example).numpy()\n",
    "print(example)\n",
    "print(encoded_example)\n",
    "print(encoded_example.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model (Logistic Regression)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Define and compile model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(encoder_int)\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=len(encoder_int.get_vocabulary()),\n",
    "                output_dim=64,\n",
    "                # user masking to handle the variable sequence lengths\n",
    "                mask_zero=True)\n",
    "model.add(embedding)\n",
    "model.output_shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vec_encoder):\n",
    "    print(len(vec_encoder.get_vocabulary()))\n",
    "    model = tf.keras.Sequential([\n",
    "        vec_encoder,\n",
    "        tf.keras.layers.Normalization(),\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=len(vec_encoder.get_vocabulary()),\n",
    "            output_dim=16,\n",
    "            # user masking to handle the variable sequence lengths\n",
    "            mask_zero=True),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Train the model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 1: Use integer indices for encoding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TENSORBOARD USAGE\n",
    "#log_dir = \"4_nlp_rnns/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(encoder_int)\n",
    "# predict on a sample formula using untrained model\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=\"adam\",\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"algorithm\": \"LogReg\",\n",
    "    \"configuration\": \"1-tokens\",\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"loss\": \"sparse_categorical_crossentropy\",\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"int\",\n",
    "    \"dataset\": \"TOKENIZED_SET_V1\"\n",
    "}\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model.fit(train_dataset, epochs=15,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=VAL_STEPS_PER_EPOCH, steps_per_epoch=STEPS_PER_EPOCH,callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 2: Use binary count vectorizer for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(encoder_multi_hot)\n",
    "# predict on a sample formula using untrained model\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"algorithm\": \"LogReg\",\n",
    "    \"configuration\": \"1-tokens\",\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"loss\": \"sparse_categorical_crossentropy\",\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"multi_hot\",\n",
    "    \"dataset\": \"TOKENIZED_SET_V1\"\n",
    "}\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "\n",
    "history = model.fit(train_dataset, epochs=15,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30,callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(encoder_count)\n",
    "# predict on a sample formula using untrained model\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"algorithm\": \"LogReg\",\n",
    "    \"configuration\": \"1-tokens\",\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"loss\": \"sparse_categorical_crossentropy\",\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"count\",\n",
    "    \"dataset\": \"TOKENIZED_SET_V1\"\n",
    "}\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "\n",
    "history = model.fit(train_dataset, epochs=15,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30,callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(encoder_bigrams)\n",
    "# predict on a sample formula using untrained model\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"algorithm\": \"LogReg\",\n",
    "    \"configuration\": \"1-tokens\",\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"loss\": \"sparse_categorical_crossentropy\",\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"bigram_count\",\n",
    "    \"dataset\": \"TOKENIZED_SET_V1\"\n",
    "}\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "\n",
    "history = model.fit(train_dataset, epochs=15,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30,callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(encoder_tf_idf)\n",
    "# predict on a sample formula using untrained model\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"algorithm\": \"LogReg\",\n",
    "    \"configuration\": \"1-tokens\",\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"loss\": \"sparse_categorical_crossentropy\",\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"tf_idf\",\n",
    "    \"dataset\": \"TOKENIZED_SET_V1\"\n",
    "}\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model.fit(train_dataset, epochs=15,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30,callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding\n",
    "sample_text = (\"g ( x ) = x ^ 3\")\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem_math_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71a9a05a8d236729134f51de1f1fd612c9215f2a378954bc400639bac96e00eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
