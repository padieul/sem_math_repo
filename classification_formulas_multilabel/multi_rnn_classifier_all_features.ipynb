{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# takes care of annoying TF-GPU warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# remove useless Tensorflow warning:\n",
    "# WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, \n",
    "# lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, \n",
    "# lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). \n",
    "# These functions will not be directly callable after loading.\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very useful for managing wandb runs: https://stackoverflow.com/questions/71106179/log-two-model-runs-with-keras-wandb\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN: Formula Label Prediction (multi-label, all features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "import ast\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import datetime\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "wandb_project_name = \"multi_label_formula_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[\"val_\"+metric], \"\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, \"val_\"+metric])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data and Preprocess Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(corpus,\n",
    "                    irrelevant_features=[\"mtype\",]):\n",
    "    # drop irrelevant columns\n",
    "    corpus.drop(irrelevant_features, inplace=True, axis=1)\n",
    "\n",
    "    def cell_str_to_list(cell_val):\n",
    "        return ast.literal_eval(cell_val)\n",
    "\n",
    "    # filter strings\n",
    "    def process_cell(cell_str):\n",
    "        stripped_f_str = cell_str[1:-1].replace(\"\\\\\\\\\", \"\\\\\")\n",
    "        f_list = stripped_f_str.split(\",\")\n",
    "        f_list = [token.replace(\"'\", \"\").replace(\" \", \"\") for token in f_list]\n",
    "        f_list = [\"{\" if token == \"\\\\{\" else token for token in f_list]\n",
    "        f_list = [\"}\" if token == \"\\\\}\" else token for token in f_list]\n",
    "        cell_str = \" \".join(f_list)\n",
    "        return cell_str\n",
    "\n",
    "    corpus[\"type_tokens\"] = corpus[\"type_tokens\"].map(process_cell)\n",
    "    corpus[\"tokens\"] = corpus[\"tokens\"].map(process_cell)\n",
    "    corpus[\"mtype_one_hot\"] = corpus[\"mtype_one_hot\"].map(cell_str_to_list)\n",
    "    corpus[\"labels\"] = corpus[\"labels\"].map(cell_str_to_list)\n",
    "    corpus = corpus.loc[(corpus[\"tokens\"].str.len() > 0) & (corpus[\"tokens\"] != \" \")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "data_p = Path(\"../data/\") / \"multi_class_unbalanced_data_TOKENIZED_V1.csv\"\n",
    "data = pd.read_csv(data_p)\n",
    "preprocess_data(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"type_tokens\"].map(lambda x: len((x.split(\" \")))).max())\n",
    "print(data[\"tokens\"].map(lambda x: len((x.split(\" \")))).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinary datasets\n",
    "SMALL_TRAIN_SIZE = 24620 - 2460\n",
    "SMALL_TEST_SIZE = 2460\n",
    "LARGE_TRAIN_SIZE = 106523 - 10650\n",
    "LARGE_TEST_SIZE = 10650\n",
    "# compact datasets\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "labels_array = np.array(data[\"labels\"].to_list())\n",
    "m_type_array = np.array(data[\"mtype_one_hot\"].to_list())\n",
    "data_as_ds = tf.data.Dataset.from_tensor_slices((data[\"tokens\"],data[\"type_tokens\"],m_type_array))\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices(labels_array)\n",
    "data_as_ds = tf.data.Dataset.zip((data_as_ds, labels_ds))\n",
    "\n",
    "test_dataset = data_as_ds.take(SMALL_TEST_SIZE)\n",
    "train_dataset = data_as_ds.skip(SMALL_TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_ds.element_spec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (example_token, example_type, example_m_type), label in train_dataset.take(5):\n",
    "    print(\"text: \", example_token.numpy())\n",
    "    print(\"type: \", example_type.numpy())\n",
    "    print(\"m_type: \", example_m_type.numpy())\n",
    "    print(\"label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2000\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCH = np.floor(SMALL_TRAIN_SIZE/BATCH_SIZE)\n",
    "VAL_STEPS_PER_EPOCH = np.floor(SMALL_TEST_SIZE/BATCH_SIZE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for int encoder\n",
    "TYPE_TOKENS_MAX_SEQ_LEN = 260\n",
    "TOKENS_MAX_SEQ_LEN = 260\n",
    "\n",
    "# for other encoders \n",
    "TYPE_TOKENS_PAD_TO_MAX_TOKENS = 80\n",
    "TOKENS_PAD_TO_MAX_TOKENS = 200\n",
    "BIGRAM_PAD_TO_MAX_TOKENS = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(output_mode_str, n_grams, mode):\n",
    "    if output_mode_str == \"int\":\n",
    "        VOCAB_SIZE = 200\n",
    "        if mode == \"token\":\n",
    "            max_seq_len = TOKENS_MAX_SEQ_LEN\n",
    "        elif mode == \"type\":\n",
    "            max_seq_len = TYPE_TOKENS_MAX_SEQ_LEN\n",
    "\n",
    "        encoder = tf.keras.layers.TextVectorization(\n",
    "            standardize=None,\n",
    "            output_mode=output_mode_str,\n",
    "            ngrams = n_grams,\n",
    "            output_sequence_length = max_seq_len,\n",
    "            split=\"whitespace\",\n",
    "            max_tokens=VOCAB_SIZE)\n",
    "        #TODO: adapt for different inputs\n",
    "        if mode == \"token\": \n",
    "            encoder.adapt(train_dataset.map(lambda inputs, label: inputs[0])) # removes the label column through transformation: text, label -> text\n",
    "        elif mode == \"type\":\n",
    "            encoder.adapt(train_dataset.map(lambda inputs, label: inputs[1])) # removes the label column through transformation: text, label -> text\n",
    "        return encoder\n",
    "    \n",
    "    if output_mode_str == \"count\" and n_grams == 2:\n",
    "        max_seq_len = BIGRAM_PAD_TO_MAX_TOKENS\n",
    "        encoder = tf.keras.layers.TextVectorization(\n",
    "            standardize=None,\n",
    "            output_mode=output_mode_str,\n",
    "            ngrams = n_grams,\n",
    "            pad_to_max_tokens = max_seq_len,\n",
    "            split=\"whitespace\",\n",
    "            max_tokens=max_seq_len)\n",
    "        \n",
    "        #TODO: adapt for different inputs\n",
    "        if mode == \"token\": \n",
    "            encoder.adapt(train_dataset.map(lambda inputs, label: inputs[0])) # removes the label column through transformation: text, label -> text\n",
    "        elif mode == \"type\":\n",
    "            encoder.adapt(train_dataset.map(lambda inputs, label: inputs[1])) # removes the label column through transformation: text, label -> text\n",
    "        return encoder\n",
    "    \n",
    "    if mode == \"token\":\n",
    "        max_seq_len = TOKENS_PAD_TO_MAX_TOKENS\n",
    "    elif mode == \"type\":\n",
    "        max_seq_len = TYPE_TOKENS_PAD_TO_MAX_TOKENS\n",
    "\n",
    "    encoder = tf.keras.layers.TextVectorization(\n",
    "        standardize=None,\n",
    "        output_mode=output_mode_str,\n",
    "        ngrams = n_grams,\n",
    "        pad_to_max_tokens = max_seq_len,\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=max_seq_len)\n",
    "    #TODO: adapt for different inputs\n",
    "    if mode == \"token\": \n",
    "        encoder.adapt(train_dataset.map(lambda inputs, label: inputs[0])) # removes the label column through transformation: text, label -> text\n",
    "    elif mode == \"type\":\n",
    "        encoder.adapt(train_dataset.map(lambda inputs, label: inputs[1])) # removes the label column through transformation: text, label -> text\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 1: Use integer indices encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_int_tokens = create_encoder(\"int\", None, \"token\")\n",
    "encoder_int_types = create_encoder(\"int\", None, \"type\")\n",
    "\n",
    "vocab_tokens = np.array(encoder_int_tokens.get_vocabulary())\n",
    "vocab_size_tokens = len(encoder_int_tokens.get_vocabulary())\n",
    "vocab_types = np.array(encoder_int_types.get_vocabulary())\n",
    "vocab_size_types = len(encoder_int_types.get_vocabulary())\n",
    "\n",
    "print(\"tokens (voc size): \", vocab_size_tokens)\n",
    "print(\"types (voc size): \", vocab_size_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example_token = encoder_int_tokens(example_token).numpy()\n",
    "encoded_example_types = encoder_int_types(example_type).numpy()\n",
    "\n",
    "print(\"tokens: \")\n",
    "print(example_token)\n",
    "print(encoded_example_token)\n",
    "print(encoded_example_token.shape)\n",
    "\n",
    "print(\"types: \")\n",
    "print(example_type)\n",
    "print(encoded_example_types)\n",
    "print(encoded_example_types.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 2: Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_count_tokens = create_encoder(\"count\", None, \"token\")\n",
    "encoder_count_types = create_encoder(\"count\", None, \"type\")\n",
    "\n",
    "vocab_tokens = np.array(encoder_count_tokens.get_vocabulary())\n",
    "vocab_size_tokens = len(encoder_count_tokens.get_vocabulary())\n",
    "vocab_types = np.array(encoder_count_types.get_vocabulary())\n",
    "vocab_size_types = len(encoder_count_types.get_vocabulary())\n",
    "\n",
    "print(\"tokens (voc size): \", vocab_size_tokens)\n",
    "print(\"types (voc size): \", vocab_size_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example_token = encoder_count_tokens(example_token).numpy()\n",
    "encoded_example_types = encoder_count_types(example_type).numpy()\n",
    "\n",
    "print(\"tokens: \")\n",
    "print(example_token)\n",
    "print(encoded_example_token)\n",
    "print(encoded_example_token.shape)\n",
    "\n",
    "print(\"types: \")\n",
    "print(example_type)\n",
    "print(encoded_example_types)\n",
    "print(encoded_example_types.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model (Bidirectional LSTM with one layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Define and compile model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(tokens_encoder, types_encoder, tokens_size, types_size, input_type, add_inp_emb_dim=1):\n",
    "    # model = create_model(encoder_count_tokens, encoder_count_types,tokens_input_len, \"float\", type_input_len)\n",
    "    embedding_input_dim = tokens_size + types_size + add_inp_emb_dim + 3 # 3 size of sem_type_model_input\n",
    "\n",
    "    tokens_model_input = tf.keras.layers.Input(dtype=tf.string, shape=(1,))\n",
    "    tokens_vectorized = tokens_encoder(tokens_model_input)\n",
    "\n",
    "    types_model_input = tf.keras.layers.Input(dtype=tf.string, shape=(1,))\n",
    "    types_vectorized = types_encoder(types_model_input)\n",
    "\n",
    "    if input_type == \"float\":\n",
    "        sem_type_model_input = tf.keras.layers.Input(dtype=tf.float32, shape=(3,))\n",
    "    elif input_type == \"int\":\n",
    "        sem_type_model_input = tf.keras.layers.Input(dtype=tf.int64, shape=(3,))\n",
    "\n",
    "    merged = tf.keras.layers.Concatenate(axis=1)([tokens_vectorized, types_vectorized, sem_type_model_input])\n",
    "\n",
    "    embedded = tf.keras.layers.Embedding(input_dim= embedding_input_dim,\n",
    "                                         output_dim=64,\n",
    "                                         # user masking to handle the variable sequence lengths\n",
    "                                         mask_zero=True)(merged)\n",
    "    bilstmed = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(embedded)\n",
    "    densed1 = tf.keras.layers.Dense(64, activation=\"relu\")(bilstmed)\n",
    "    model_output = tf.keras.layers.Dense(40)(densed1)\n",
    "\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[tokens_model_input, types_model_input, sem_type_model_input], outputs=model_output)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Train the model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 1: Use integer indices for encoding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TENSORBOARD USAGE\n",
    "#log_dir = \"4_nlp_rnns/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input_len = TOKENS_MAX_SEQ_LEN \n",
    "type_input_len = TYPE_TOKENS_MAX_SEQ_LEN\n",
    "\n",
    "model = create_model(encoder_int_tokens, encoder_int_types,tokens_input_len, type_input_len, \"int\")\n",
    "\"\"\"\n",
    "sample_text = \"( A \\cup B ) \\cap (C \\cup D )\"\n",
    "# predict on a sample formula using untrained model\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 70\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "    \"configuration\": \"small-ordinary-unbalanced-all-inputs\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"int\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V1\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict on a sample text without padding\n",
    "\"\"\"\n",
    "predictions = model.predict((np.array([example_token]), np.array([example_type])))\n",
    "print(predictions[0])\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input_len = TOKENS_PAD_TO_MAX_TOKENS \n",
    "type_input_len = TYPE_TOKENS_PAD_TO_MAX_TOKENS\n",
    "\n",
    "model = create_model(encoder_count_tokens, encoder_count_types,tokens_input_len, type_input_len, \"float\") # maybe add 30\n",
    "# predict on a sample formula using untrained model\n",
    "#predictions = model.predict(np.array([sample_text]))\n",
    "#print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 80\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "    \"configuration\": \"small-ordinary-unbalanced-all-inputs\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"count\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V1\"\n",
    "}\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset,\n",
    "                    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, \"loss\")\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# predict on a sample text without padding\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem_math_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71a9a05a8d236729134f51de1f1fd612c9215f2a378954bc400639bac96e00eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
