{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# takes care of annoying TF-GPU warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# remove useless Tensorflow warning:\n",
    "# WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, \n",
    "# lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, \n",
    "# lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). \n",
    "# These functions will not be directly callable after loading.\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very useful for managing wandb runs: https://stackoverflow.com/questions/71106179/log-two-model-runs-with-keras-wandb\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multi_classifier_utils as mc_u"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN (BiLSTM): Formula Label Prediction (multi-label, all features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "import ast\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import datetime\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "wandb_project_name = \"multi_label_formula_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[\"val_\"+metric], \"\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, \"val_\"+metric])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data and Preprocess Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(corpus,\n",
    "                    irrelevant_features=[\"mtype\",]):\n",
    "    # drop irrelevant columns\n",
    "    corpus.drop(irrelevant_features, inplace=True, axis=1)\n",
    "\n",
    "    def cell_str_to_list(cell_val):\n",
    "        return ast.literal_eval(cell_val)\n",
    "\n",
    "    # filter strings\n",
    "    def process_cell(cell_str):\n",
    "        stripped_f_str = cell_str[1:-1].replace(\"\\\\\\\\\", \"\\\\\")\n",
    "        f_list = stripped_f_str.split(\",\")\n",
    "        f_list = [token.replace(\"'\", \"\").replace(\" \", \"\") for token in f_list]\n",
    "        f_list = [\"{\" if token == \"\\\\{\" else token for token in f_list]\n",
    "        f_list = [\"}\" if token == \"\\\\}\" else token for token in f_list]\n",
    "        cell_str = \" \".join(f_list)\n",
    "        return cell_str\n",
    "\n",
    "    corpus[\"type_tokens\"] = corpus[\"type_tokens\"].map(process_cell)\n",
    "    corpus[\"tokens\"] = corpus[\"tokens\"].map(process_cell)\n",
    "    corpus[\"mtype_one_hot\"] = corpus[\"mtype_one_hot\"].map(cell_str_to_list)\n",
    "    corpus[\"labels\"] = corpus[\"labels\"].map(cell_str_to_list)\n",
    "    corpus = corpus.loc[(corpus[\"tokens\"].str.len() > 0) & (corpus[\"tokens\"] != \" \")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "data_p = Path(\"../data/\") / \"multi_class_unbalanced_data_TOKENIZED_V1.csv\"\n",
    "data = pd.read_csv(data_p)\n",
    "preprocess_data(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"type_tokens\"].map(lambda x: len((x.split(\" \")))).max())\n",
    "print(data[\"tokens\"].map(lambda x: len((x.split(\" \")))).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinary datasets\n",
    "SMALL_TRAIN_SIZE = 24620 - 2460\n",
    "SMALL_TEST_SIZE = 2460\n",
    "LARGE_TRAIN_SIZE = 106523 - 10650\n",
    "LARGE_TEST_SIZE = 10650\n",
    "# compact datasets\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "labels_array = np.array(data[\"labels\"].to_list())\n",
    "m_type_array = np.array(data[\"mtype_one_hot\"].to_list())\n",
    "\n",
    "dataset4_all_features = tf.data.Dataset.from_tensor_slices((data[\"tokens\"],data[\"type_tokens\"],m_type_array), name=\"data\")\n",
    "dataset3_tokens_types = tf.data.Dataset.from_tensor_slices((data[\"tokens\"],data[\"type_tokens\"]), name=\"data\")\n",
    "dataset2_types = tf.data.Dataset.from_tensor_slices((data[\"type_tokens\"]), name=\"data\")\n",
    "dataset1_tokens = tf.data.Dataset.from_tensor_slices((data[\"tokens\"]), name=\"data\")\n",
    "\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices(labels_array, name=\"label\")\n",
    "#data_as_ds = tf.data.Dataset.zip((dat_as_ds, labels_ds))\n",
    "\n",
    "dataset1_tokens_l = tf.data.Dataset.zip((dataset1_tokens, labels_ds))\n",
    "dataset2_types_l = tf.data.Dataset.zip((dataset2_types, labels_ds))\n",
    "dataset3_tokens_types_l = tf.data.Dataset.zip((dataset3_tokens_types, labels_ds))\n",
    "dataset4_all_features_l = tf.data.Dataset.zip((dataset4_all_features, labels_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset1 = dataset1_tokens_l.take(SMALL_TEST_SIZE)\n",
    "train_dataset1 = dataset1_tokens_l.skip(SMALL_TEST_SIZE)\n",
    "test_dataset2 = dataset2_types_l.take(SMALL_TEST_SIZE)\n",
    "train_dataset2 = dataset2_types_l.skip(SMALL_TEST_SIZE)\n",
    "test_dataset3 = dataset3_tokens_types_l.take(SMALL_TEST_SIZE)\n",
    "train_dataset3 = dataset3_tokens_types_l.skip(SMALL_TEST_SIZE)\n",
    "test_dataset4 = dataset4_all_features_l.take(SMALL_TEST_SIZE)\n",
    "train_dataset4 = dataset4_all_features_l.skip(SMALL_TEST_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (example_token, example_type, example_m_type), label in train_dataset4.take(5):\n",
    "    print(\"text: \", example_token.numpy())\n",
    "    print(\"type: \", example_type.numpy())\n",
    "    print(\"m_type: \", example_m_type.numpy())\n",
    "    print(\"label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2000\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCH = np.floor(SMALL_TRAIN_SIZE/BATCH_SIZE)\n",
    "VAL_STEPS_PER_EPOCH = np.floor(SMALL_TEST_SIZE/BATCH_SIZE)\n",
    "#train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "#test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset1 = test_dataset1.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "train_dataset1 = train_dataset1.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset2 = test_dataset2.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "train_dataset2 = train_dataset2.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset3 = test_dataset3.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "train_dataset3 = train_dataset3.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset4 = test_dataset4.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "train_dataset4 = train_dataset4.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for int encoder\n",
    "TYPE_TOKENS_MAX_SEQ_LEN = 260\n",
    "TOKENS_MAX_SEQ_LEN = 260\n",
    "\n",
    "\n",
    "# for other encoders \n",
    "TYPE_TOKENS_PAD_TO_MAX_TOKENS = 80\n",
    "TOKENS_PAD_TO_MAX_TOKENS = 200\n",
    "BIGRAM_PAD_TO_MAX_TOKENS = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_encoder(encoder, mode, dataset, dataset_type):\n",
    "    if mode == \"token\": \n",
    "\n",
    "        if dataset_type == 1:\n",
    "            encoder.adapt(dataset.map(lambda inputs, label: inputs))\n",
    "        elif dataset_type == 2:\n",
    "            ...\n",
    "        elif dataset_type == 3:\n",
    "            encoder.adapt(dataset.map(lambda inputs, label: inputs[0])) # removes the label column through transformation: text, label -> text\n",
    "        elif dataset_type == 4:\n",
    "            encoder.adapt(dataset.map(lambda inputs, label: inputs[0])) # removes the label column through transformation: text, label -> text  \n",
    "    elif mode == \"type\":\n",
    "        \n",
    "        if dataset_type == 1:\n",
    "            ...\n",
    "        elif dataset_type == 2:\n",
    "            encoder.adapt(dataset.map(lambda inputs, label: inputs))\n",
    "        elif dataset_type == 3:\n",
    "            encoder.adapt(dataset.map(lambda inputs, label: inputs[1])) # removes the label column through transformation: text, label -> text\n",
    "        elif dataset_type == 4:\n",
    "            encoder.adapt(dataset.map(lambda inputs, label: inputs[1])) # removes the label column through transformation: text, label -> text\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(output_mode_str, n_grams, mode, dataset, dataset_type):\n",
    "    if output_mode_str == \"int\":\n",
    "        VOCAB_SIZE = 200\n",
    "        if mode == \"token\":\n",
    "            max_seq_len = TOKENS_MAX_SEQ_LEN\n",
    "        elif mode == \"type\":\n",
    "            max_seq_len = TYPE_TOKENS_MAX_SEQ_LEN\n",
    "\n",
    "        encoder = tf.keras.layers.TextVectorization(\n",
    "            standardize=None,\n",
    "            output_mode=output_mode_str,\n",
    "            ngrams = n_grams,\n",
    "            output_sequence_length = max_seq_len,\n",
    "            split=\"whitespace\",\n",
    "            max_tokens=VOCAB_SIZE)\n",
    "        #TODO: adapt for different inputs\n",
    "        encoder = adapt_encoder(encoder, mode, dataset, dataset_type)\n",
    "        return encoder\n",
    "    \n",
    "    if output_mode_str == \"count\" and n_grams == 2:\n",
    "        max_seq_len = BIGRAM_PAD_TO_MAX_TOKENS\n",
    "        encoder = tf.keras.layers.TextVectorization(\n",
    "            standardize=None,\n",
    "            output_mode=output_mode_str,\n",
    "            ngrams = n_grams,\n",
    "            pad_to_max_tokens = max_seq_len,\n",
    "            split=\"whitespace\",\n",
    "            max_tokens=max_seq_len)\n",
    "        \n",
    "        \n",
    "        encoder = adapt_encoder(encoder, mode, dataset, dataset_type)\n",
    "        return encoder\n",
    "    \n",
    "    if mode == \"token\":\n",
    "        max_seq_len = TOKENS_PAD_TO_MAX_TOKENS\n",
    "    elif mode == \"type\":\n",
    "        max_seq_len = TYPE_TOKENS_PAD_TO_MAX_TOKENS\n",
    "\n",
    "    encoder = tf.keras.layers.TextVectorization(\n",
    "        standardize=None,\n",
    "        output_mode=output_mode_str,\n",
    "        ngrams = n_grams,\n",
    "        pad_to_max_tokens = max_seq_len,\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=max_seq_len)\n",
    "    #TODO: adapt for different inputs\n",
    "    encoder = adapt_encoder(encoder, mode, dataset, dataset_type)\n",
    "    return encoder\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 1: Use integer indices encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_int_tokens = create_encoder(\"int\", None, \"token\", train_dataset1, 1)\n",
    "encoder_int_types = create_encoder(\"int\", None, \"type\", train_dataset2, 2)\n",
    "\n",
    "vocab_tokens = np.array(encoder_int_tokens.get_vocabulary())\n",
    "vocab_size_tokens = len(encoder_int_tokens.get_vocabulary())\n",
    "vocab_types = np.array(encoder_int_types.get_vocabulary())\n",
    "vocab_size_types = len(encoder_int_types.get_vocabulary())\n",
    "\n",
    "print(\"tokens (voc size): \", vocab_size_tokens)\n",
    "print(\"types (voc size): \", vocab_size_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example_token = encoder_int_tokens(example_token).numpy()\n",
    "encoded_example_types = encoder_int_types(example_type).numpy()\n",
    "\n",
    "print(\"tokens: \")\n",
    "print(example_token)\n",
    "print(encoded_example_token)\n",
    "print(encoded_example_token.shape)\n",
    "\n",
    "print(\"types: \")\n",
    "print(example_type)\n",
    "print(encoded_example_types)\n",
    "print(encoded_example_types.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representation 2: Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_count_tokens = create_encoder(\"count\", None, \"token\", train_dataset1, 1)\n",
    "encoder_count_types = create_encoder(\"count\", None, \"type\", train_dataset2, 2)\n",
    "\n",
    "vocab_tokens = np.array(encoder_count_tokens.get_vocabulary())\n",
    "vocab_size_tokens = len(encoder_count_tokens.get_vocabulary())\n",
    "vocab_types = np.array(encoder_count_types.get_vocabulary())\n",
    "vocab_size_types = len(encoder_count_types.get_vocabulary())\n",
    "\n",
    "print(\"tokens (voc size): \", vocab_size_tokens)\n",
    "print(\"types (voc size): \", vocab_size_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example_token = encoder_count_tokens(example_token).numpy()\n",
    "encoded_example_types = encoder_count_types(example_type).numpy()\n",
    "\n",
    "print(\"tokens: \")\n",
    "print(example_token)\n",
    "print(encoded_example_token)\n",
    "print(encoded_example_token.shape)\n",
    "\n",
    "print(\"types: \")\n",
    "print(example_type)\n",
    "print(encoded_example_types)\n",
    "print(encoded_example_types.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model (RNN(BiLSTM))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Train the model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: Use integer indices for encoding tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1: ONLY TOKENS\n",
    "Find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input_len = TOKENS_MAX_SEQ_LEN \n",
    "type_input_len = TYPE_TOKENS_MAX_SEQ_LEN\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "encoder_int_tokens1 = create_encoder(\"int\", None, \"token\", train_dataset1, 1)\n",
    "model_builder1 = mc_u.create_model_builder1_RNN(NUM_CLASSES, encoder_int_tokens1, tokens_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder1,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     max_epochs=15,\n",
    "                     factor=3,\n",
    "                     directory='meta_dir/model1_lr',\n",
    "                     project_name='model1_lr')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset1,\n",
    "             epochs=30,\n",
    "             validation_data=test_dataset1,\n",
    "             #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "             validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "             callbacks= [stop_early])#[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "optimal_lr = best_hps.get(\"learning_rate\")\n",
    "optimal_emb_dims = best_hps.get(\"emb_dims\")\n",
    "optimal_lstm_units = best_hps.get(\"lstm_units\")\n",
    "optimal_dense_units = best_hps.get(\"dense_units\")\n",
    "\n",
    "print(optimal_lr)\n",
    "print(optimal_emb_dims)\n",
    "print(optimal_lstm_units)\n",
    "print(optimal_dense_units)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "model1 = mc_u.create_model1_LR(NUM_CLASSES, optimal_emb_dims, optimal_lstm_units, optimal_dense_units, encoder_int_tokens1, tokens_input_len)\n",
    "model1.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"lstm_units\": optimal_lstm_units, \n",
    "    \"dense_units\": optimal_dense_units,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "    \"configuration\": \"multi_only_tokens\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"int\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V1\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model1.fit(train_dataset1, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset1,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2: ONLY TYPES\n",
    "Find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input_len = TOKENS_MAX_SEQ_LEN \n",
    "type_input_len = TYPE_TOKENS_MAX_SEQ_LEN\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "encoder_int_types2 = create_encoder(\"int\", None, \"type\", train_dataset2, 2)    \n",
    "model_builder2 = mc_u.create_model_builder2_RNN(NUM_CLASSES, encoder_int_types2,type_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder2,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     max_epochs=15,\n",
    "                     factor=3,\n",
    "                     directory='meta_dir/model2_lr',\n",
    "                     project_name='model2_lr')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset2,\n",
    "             epochs=30,\n",
    "             validation_data=test_dataset2,\n",
    "             #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "             validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "             callbacks= [stop_early])#[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "optimal_lr = best_hps.get(\"learning_rate\")\n",
    "optimal_emb_dims = best_hps.get(\"emb_dims\")\n",
    "optimal_lstm_units = best_hps.get(\"lstm_units\")\n",
    "optimal_dense_units = best_hps.get(\"dense_units\")\n",
    "\n",
    "print(optimal_lr)\n",
    "print(optimal_emb_dims)\n",
    "print(optimal_lstm_units)\n",
    "print(optimal_dense_units)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "model2 = mc_u.create_model2_LR(NUM_CLASSES, optimal_emb_dims, optimal_lstm_units, optimal_dense_units, encoder_int_types2, type_input_len)\n",
    "model2.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"lstm_units\": optimal_lstm_units, \n",
    "    \"dense_units\": optimal_dense_units,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "    \"configuration\": \"multi_only_types\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"int\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V1\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model2.fit(train_dataset2, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset2,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3: TOKENS AND TYPES\n",
    "Find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input_len = TOKENS_MAX_SEQ_LEN \n",
    "type_input_len = TYPE_TOKENS_MAX_SEQ_LEN\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "encoder_int_tokens3 = create_encoder(\"int\", None, \"token\", train_dataset3, 3)\n",
    "encoder_int_types3 = create_encoder(\"int\", None, \"type\", train_dataset3, 3)\n",
    "model_builder3 = mc_u.create_model_builder3_RNN(NUM_CLASSES, encoder_int_tokens3,encoder_int_types3, tokens_input_len, type_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder3,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     max_epochs=15,\n",
    "                     factor=3,\n",
    "                     directory='meta_dir/model3_lr',\n",
    "                     project_name='model3_lr')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset3,\n",
    "             epochs=30,\n",
    "             validation_data=test_dataset3,\n",
    "             #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "             validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "             callbacks= [stop_early])#[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "optimal_lr = best_hps.get(\"learning_rate\")\n",
    "optimal_emb_dims = best_hps.get(\"emb_dims\")\n",
    "optimal_lstm_units = best_hps.get(\"lstm_units\")\n",
    "optimal_dense_units = best_hps.get(\"dense_units\")\n",
    "\n",
    "print(optimal_lr)\n",
    "print(optimal_emb_dims)\n",
    "print(optimal_lstm_units)\n",
    "print(optimal_dense_units)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "model3 = mc_u.create_model3_LR(NUM_CLASSES, optimal_emb_dims, optimal_lstm_units, optimal_dense_units, encoder_int_tokens3, encoder_int_types3, tokens_input_len, type_input_len)\n",
    "model3.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"lstm_units\": optimal_lstm_units, \n",
    "    \"dense_units\": optimal_dense_units,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "    \"configuration\": \"multi_tokens_types\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"int\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V1\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model3.fit(train_dataset3, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset3,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 4: TOKENS AND TYPES AND SEM MATH LABELS\n",
    "Find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input_len = TOKENS_MAX_SEQ_LEN \n",
    "type_input_len = TYPE_TOKENS_MAX_SEQ_LEN\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "encoder_int_tokens4 = create_encoder(\"int\", None, \"token\", train_dataset4, 4)\n",
    "encoder_int_types4 = create_encoder(\"int\", None, \"type\", train_dataset4, 4)\n",
    "model_builder4 = mc_u.create_model_builder4_RNN(NUM_CLASSES, encoder_int_tokens4,encoder_int_types4, tokens_input_len, type_input_len, \"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder4,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     max_epochs=15,\n",
    "                     factor=3,\n",
    "                     directory='meta_dir/model4_lr',\n",
    "                     project_name='model4_lr')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset4,\n",
    "             epochs=30,\n",
    "             validation_data=test_dataset4,\n",
    "             #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "             validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "             callbacks= [stop_early])#[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "optimal_lr = best_hps.get(\"learning_rate\")\n",
    "optimal_emb_dims = best_hps.get(\"emb_dims\")\n",
    "optimal_lstm_units = best_hps.get(\"lstm_units\")\n",
    "optimal_dense_units = best_hps.get(\"dense_units\")\n",
    "\n",
    "print(optimal_lr)\n",
    "print(optimal_emb_dims)\n",
    "print(optimal_lstm_units)\n",
    "print(optimal_dense_units)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "model4 = mc_u.create_model4_LR(NUM_CLASSES, optimal_emb_dims, optimal_lstm_units, optimal_dense_units, encoder_int_tokens4, encoder_int_types4, tokens_input_len, type_input_len, \"int\")\n",
    "model4.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"lstm_units\": optimal_lstm_units, \n",
    "    \"dense_units\": optimal_dense_units,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "    \"configuration\": \"multi_all_features\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"int\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V1\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model4.fit(train_dataset4, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset4,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2: Use count vectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1: ONLY TOKENS\n",
    "Find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input_len = TOKENS_MAX_SEQ_LEN \n",
    "type_input_len = TYPE_TOKENS_MAX_SEQ_LEN\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "encoder_count_tokens1 = create_encoder(\"count\", None, \"token\", train_dataset1, 1)\n",
    "model_builder1c = mc_u.create_model_builder1_RNN(NUM_CLASSES, encoder_count_tokens1,tokens_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder1c,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     max_epochs=15,\n",
    "                     factor=3,\n",
    "                     directory='meta_dir/model1c_lr',\n",
    "                     project_name='model1c_lr')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset1,\n",
    "             epochs=30,\n",
    "             validation_data=test_dataset1,\n",
    "             #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "             validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "             callbacks= [stop_early])#[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "optimal_lr = best_hps.get(\"learning_rate\")\n",
    "optimal_emb_dims = best_hps.get(\"emb_dims\")\n",
    "optimal_lstm_units = best_hps.get(\"lstm_units\")\n",
    "optimal_dense_units = best_hps.get(\"dense_units\")\n",
    "\n",
    "print(optimal_lr)\n",
    "print(optimal_emb_dims)\n",
    "print(optimal_lstm_units)\n",
    "print(optimal_dense_units)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "model1c = mc_u.create_model1_LR(NUM_CLASSES, optimal_emb_dims, optimal_lstm_units, optimal_dense_units, encoder_count_tokens, tokens_input_len)\n",
    "model1c.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"lstm_units\": optimal_lstm_units, \n",
    "    \"dense_units\": optimal_dense_units,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "    \"configuration\": \"multi_only_tokens\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"count\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V1\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model1c.fit(train_dataset1, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset1,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2: ONLY TYPES\n",
    "Find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input_len = TOKENS_MAX_SEQ_LEN \n",
    "type_input_len = TYPE_TOKENS_MAX_SEQ_LEN\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "encoder_count_types = create_encoder(\"count\", None, \"type\", train_dataset2, 2)\n",
    "model_builder2c = mc_u.create_model_builder2_RNN(NUM_CLASSES, encoder_count_types, type_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder2c,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     max_epochs=15,\n",
    "                     factor=3,\n",
    "                     directory='meta_dir/model2c_lr',\n",
    "                     project_name='model2c_lr')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset2,\n",
    "             epochs=30,\n",
    "             validation_data=test_dataset2,\n",
    "             #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "             validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "             callbacks= [stop_early])#[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "optimal_lr = best_hps.get(\"learning_rate\")\n",
    "optimal_emb_dims = best_hps.get(\"emb_dims\")\n",
    "optimal_lstm_units = best_hps.get(\"lstm_units\")\n",
    "optimal_dense_units = best_hps.get(\"dense_units\")\n",
    "\n",
    "print(optimal_lr)\n",
    "print(optimal_emb_dims)\n",
    "print(optimal_lstm_units)\n",
    "print(optimal_dense_units)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "model2c = mc_u.create_model2_LR(NUM_CLASSES, optimal_emb_dims, optimal_lstm_units, optimal_dense_units, encoder_count_types, type_input_len)\n",
    "model2c.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"lstm_units\": optimal_lstm_units, \n",
    "    \"dense_units\": optimal_dense_units,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "    \"configuration\": \"multi_only_types\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"count\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V1\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model2c.fit(train_dataset2, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset2,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3: TOKENS AND TYPES\n",
    "Find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input_len = TOKENS_MAX_SEQ_LEN \n",
    "type_input_len = TYPE_TOKENS_MAX_SEQ_LEN\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "encoder_count_tokens3 = create_encoder(\"count\", None, \"token\", train_dataset3, 3)\n",
    "encoder_count_types3 = create_encoder(\"count\", None, \"type\", train_dataset3, 3)\n",
    "model_builder3c = mc_u.create_model_builder3_RNN(NUM_CLASSES, encoder_count_tokens3,encoder_count_types3, tokens_input_len, type_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder3c,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     max_epochs=15,\n",
    "                     factor=3,\n",
    "                     directory='meta_dir/model3c_lr',\n",
    "                     project_name='model3c_lr')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset3,\n",
    "             epochs=30,\n",
    "             validation_data=test_dataset3,\n",
    "             #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "             validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "             callbacks= [stop_early])#[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "optimal_lr = best_hps.get(\"learning_rate\")\n",
    "optimal_emb_dims = best_hps.get(\"emb_dims\")\n",
    "optimal_lstm_units = best_hps.get(\"lstm_units\")\n",
    "optimal_dense_units = best_hps.get(\"dense_units\")\n",
    "\n",
    "print(optimal_lr)\n",
    "print(optimal_emb_dims)\n",
    "print(optimal_lstm_units)\n",
    "print(optimal_dense_units)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "model3c = mc_u.create_model3_LR(NUM_CLASSES, optimal_emb_dims, optimal_lstm_units, optimal_dense_units, encoder_count_tokens3, encoder_count_types3, tokens_input_len, type_input_len)\n",
    "model3c.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"lstm_units\": optimal_lstm_units, \n",
    "    \"dense_units\": optimal_dense_units,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "    \"configuration\": \"multi_tokens_types\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"count\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V1\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model3c.fit(train_dataset3, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset3,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 4: TOKENS, TYPES, SEM MATH LABELS\n",
    "Find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input_len = TOKENS_MAX_SEQ_LEN \n",
    "type_input_len = TYPE_TOKENS_MAX_SEQ_LEN\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "encoder_count_tokens4 = create_encoder(\"count\", None, \"token\", train_dataset4, 4)\n",
    "encoder_count_types4 = create_encoder(\"count\", None, \"type\", train_dataset4, 4)\n",
    "model_builder4c = mc_u.create_model_builder4_RNN(NUM_CLASSES, encoder_count_tokens4, encoder_count_types4, tokens_input_len, type_input_len, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder4c,\n",
    "                     objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "                     max_epochs=15,\n",
    "                     factor=3,\n",
    "                     directory='meta_dir/model4c_lr',\n",
    "                     project_name='model4c_lr')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset4,\n",
    "             epochs=30,\n",
    "             validation_data=test_dataset4,\n",
    "             #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "             validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "             callbacks= [stop_early])#[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "optimal_lr = best_hps.get(\"learning_rate\")\n",
    "optimal_emb_dims = best_hps.get(\"emb_dims\")\n",
    "optimal_lstm_units = best_hps.get(\"lstm_units\")\n",
    "optimal_dense_units = best_hps.get(\"dense_units\")\n",
    "\n",
    "print(optimal_lr)\n",
    "print(optimal_emb_dims)\n",
    "print(optimal_lstm_units)\n",
    "print(optimal_dense_units)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "model4c = mc_u.create_model4_LR(NUM_CLASSES, optimal_emb_dims, optimal_lstm_units, optimal_dense_units, encoder_count_tokens4, encoder_count_types4, tokens_input_len, type_input_len, \"float\")\n",
    "model4c.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"lstm_units\": optimal_lstm_units, \n",
    "    \"dense_units\": optimal_dense_units,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "    \"configuration\": \"multi_all_features\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"count\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V1\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model4c.fit(train_dataset4, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset4,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem_math_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71a9a05a8d236729134f51de1f1fd612c9215f2a378954bc400639bac96e00eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
