{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# takes care of annoying TF-GPU warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# remove useless Tensorflow warning:\n",
    "# WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, \n",
    "# lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, \n",
    "# lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). \n",
    "# These functions will not be directly callable after loading.\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very useful for managing wandb runs: https://stackoverflow.com/questions/71106179/log-two-model-runs-with-keras-wandb\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Training (Log Reg and BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "import ast\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import datetime\n",
    "import io\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "wandb_project_name = \"formula_embedding_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_token_embeddings(model, encoder_int_tokens, embedding_layer_name, class_task_str, emb_dim_str):\n",
    "\n",
    "    vec_path = Path(\"embedding_vecs_binary/\") / (class_task_str + \"_\" + emb_dim_str + \"_\" + \"vectors.tsv\")\n",
    "    meta_path = Path(\"embedding_vecs_binary/\") / (class_task_str + \"_\" + emb_dim_str + \"_\" + \"metadata.tsv\")\n",
    "\n",
    "    out_v = io.open(vec_path, 'w', encoding='utf-8')\n",
    "    out_m = io.open(meta_path, 'w', encoding='utf-8')\n",
    "    weights = model.get_layer(embedding_layer_name).get_weights()[0]\n",
    "    vocab = encoder_int_tokens.get_vocabulary()\n",
    "\n",
    "    for index, word in enumerate(vocab):\n",
    "        if index == 0:\n",
    "            continue  # skip 0, it's padding.\n",
    "        vec = weights[index]\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "        out_m.write(word + \"\\n\")\n",
    "    out_v.close()\n",
    "    out_m.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data and Preprocess Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(corpus,\n",
    "                    irrelevant_features=[\"mtype\",]):\n",
    "    # drop irrelevant columns\n",
    "    corpus.drop(irrelevant_features, inplace=True, axis=1)\n",
    "\n",
    "    def cell_str_to_list(cell_val):\n",
    "        return ast.literal_eval(cell_val)\n",
    "\n",
    "    # filter strings\n",
    "    def process_cell(cell_str):\n",
    "        stripped_f_str = cell_str[1:-1].replace(\"\\\\\\\\\", \"\\\\\")\n",
    "        f_list = stripped_f_str.split(\",\")\n",
    "        f_list = [token.replace(\"'\", \"\").replace(\" \", \"\") for token in f_list]\n",
    "        f_list = [\"{\" if token == \"\\\\{\" else token for token in f_list]\n",
    "        f_list = [\"}\" if token == \"\\\\}\" else token for token in f_list]\n",
    "        cell_str = \" \".join(f_list)\n",
    "        return cell_str\n",
    "\n",
    "    corpus[\"type_tokens\"] = corpus[\"type_tokens\"].map(process_cell)\n",
    "    corpus[\"tokens\"] = corpus[\"tokens\"].map(process_cell)\n",
    "    corpus[\"mtype_one_hot\"] = corpus[\"mtype_one_hot\"].map(cell_str_to_list)\n",
    "    corpus[\"labels\"] = corpus[\"labels\"].map(cell_str_to_list)\n",
    "    corpus = corpus.loc[(corpus[\"tokens\"].str.len() > 0) & (corpus[\"tokens\"] != \" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "data_p = Path(\"../data/\") / \"multi_class_unbalanced_data_TOKENIZED_V2.csv\"\n",
    "data = pd.read_csv(data_p)\n",
    "preprocess_data(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_TRAIN_SIZE = 106523 - 10650\n",
    "LARGE_TEST_SIZE = 10650\n",
    "# compact datasets\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "dataset1_tokens = tf.data.Dataset.from_tensor_slices((data[\"tokens\"]), name=\"data\")\n",
    "labels_array = np.array(data[\"labels\"].to_list())\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices(labels_array, name=\"label\")\n",
    "dataset1_tokens_l = tf.data.Dataset.zip((dataset1_tokens, labels_ds))\n",
    "test_dataset1 = dataset1_tokens_l.take(LARGE_TEST_SIZE)\n",
    "train_dataset1 = dataset1_tokens_l.skip(LARGE_TEST_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_token,  label in train_dataset1.take(5):\n",
    "    print(\"text: \", example_token.numpy())\n",
    "    print(\"label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2000\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCH = np.floor(LARGE_TRAIN_SIZE/BATCH_SIZE)\n",
    "VAL_STEPS_PER_EPOCH = np.floor(LARGE_TEST_SIZE/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset1 = test_dataset1.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "train_dataset1 = train_dataset1.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Encoding (integer indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(output_mode_str, n_grams):\n",
    "    VOCAB_SIZE = 1000\n",
    "    encoder = tf.keras.layers.TextVectorization(\n",
    "        standardize=None,\n",
    "        output_mode=output_mode_str,\n",
    "        ngrams = n_grams,\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=VOCAB_SIZE)\n",
    "    encoder.adapt(train_dataset1.map(lambda tokens, label: tokens)) # removes the label column through transformation: text, label -> text\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_int_tokens = create_encoder(\"int\", 1)\n",
    "\n",
    "vocab_tokens = np.array(encoder_int_tokens.get_vocabulary())\n",
    "vocab_tokens[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder_int_tokens(example_token).numpy()\n",
    "print(example_token)\n",
    "print(encoded_example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: LR, emb dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 150\n",
    "optimal_lr = 0.0001\n",
    "optimal_emb_dims = 64\n",
    "dp1 = 0.2\n",
    "dp2 = 0.3\n",
    "tokens_input_len = len(encoder_int_tokens.get_vocabulary()) + 1\n",
    "\n",
    "model1 = mc_u.create_model1_LR(NUM_CLASSES, optimal_emb_dims, dp1, dp2, encoder_int_tokens, tokens_input_len)\n",
    "model1.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"dp1\": dp1, \n",
    "    \"dp2\": dp2,\n",
    "    \"algorithm\": \"LogReg\",\n",
    "    \"configuration\": \"multi_only_tokens\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"int\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V2\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model1.fit(train_dataset1, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset1,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take model -> save embedding vecs\n",
    "mc_u.save_token_embeddings(model1, encoder_int_tokens, \"embedding\", \"multilabel\", str(optimal_emb_dims))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2: LR, emb dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 150\n",
    "optimal_lr = 0.0001\n",
    "optimal_emb_dims = 128\n",
    "dp1 = 0.2\n",
    "dp2 = 0.3\n",
    "tokens_input_len = len(encoder_int_tokens.get_vocabulary()) + 1\n",
    "\n",
    "model2 = mc_u.create_model1_LR(NUM_CLASSES, optimal_emb_dims, dp1, dp2, encoder_int_tokens, tokens_input_len)\n",
    "model2.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"dp1\": dp1, \n",
    "    \"dp2\": dp2,\n",
    "    \"algorithm\": \"LogReg\",\n",
    "    \"configuration\": \"multi_only_tokens\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"int\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V2\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model2.fit(train_dataset1, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset1,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take model -> save embedding vecs\n",
    "mc_u.save_token_embeddings(model2, encoder_int_tokens, \"embedding\", \"multilabel\", str(optimal_emb_dims))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3: RNN, emb dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "optimal_lr = 0.0001\n",
    "optimal_emb_dims = 64\n",
    "lstm_units = 64\n",
    "dense_units = 64\n",
    "tokens_input_len = len(encoder_int_tokens.get_vocabulary()) + 1\n",
    "\n",
    "model3 = mc_u.create_model1_RNN(NUM_CLASSES, optimal_emb_dims, lstm_units, dense_units, encoder_int_tokens, tokens_input_len)\n",
    "model3.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"lstm_units\": lstm_units, \n",
    "    \"dense_units\": dense_units,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "     \n",
    "    \"configuration\": \"multi_only_tokens\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"int\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V2\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model3.fit(train_dataset1, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset1,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take model -> save embedding vecs\n",
    "mc_u.save_token_embeddings(model3, encoder_int_tokens, \"embedding\", \"multilabel\", str(optimal_emb_dims))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 4: RNN, emb dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "optimal_lr = 0.0001\n",
    "optimal_emb_dims = 128\n",
    "lstm_units = 64\n",
    "dense_units = 64\n",
    "tokens_input_len = len(encoder_int_tokens.get_vocabulary()) + 1\n",
    "\n",
    "model4 = mc_u.create_model1_RNN(NUM_CLASSES, optimal_emb_dims, lstm_units, dense_units, encoder_int_tokens, tokens_input_len)\n",
    "model4.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(optimal_lr),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"learning_rate\": optimal_lr,\n",
    "    \"emb_dim\": optimal_emb_dims,\n",
    "    \"lstm_units\": lstm_units, \n",
    "    \"dense_units\": dense_units,\n",
    "    \"algorithm\": \"BiLstm\",\n",
    "     \n",
    "    \"configuration\": \"multi_only_tokens\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": 64,\n",
    "    \"vectorizer\": \"int\",\n",
    "    \"dataset\": \"multi_class_unbalanced_data_TOKENIZED_V2\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=wandb_project_name, reinit=True, config=configs_dict)\n",
    "\n",
    "history = model4.fit(train_dataset1, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=test_dataset1,\n",
    "                    #steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                    callbacks= [WandbCallback()])#[tensorboard_callback])\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take model -> save embedding vecs\n",
    "mc_u.save_token_embeddings(model4, encoder_int_tokens, \"embedding\", \"multilabel\", str(optimal_emb_dims))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem_math_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
